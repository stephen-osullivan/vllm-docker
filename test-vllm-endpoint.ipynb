{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu May  9 10:56:00 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.40.06              Driver Version: 551.23         CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3070 ...    On  |   00000000:01:00.0  On |                  N/A |\n",
      "| N/A   47C    P8             14W /  115W |    7842MiB /   8192MiB |      4%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A         1      C   /python3.10                                 N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"object\":\"list\",\"data\":[{\"id\":\"TheBloke/Mistral-7B-Instruct-v0.2-AWQ\",\"object\":\"model\",\"created\":1715248640,\"owned_by\":\"vllm\",\"root\":\"TheBloke/Mistral-7B-Instruct-v0.2-AWQ\",\"parent\":null,\"permission\":[{\"id\":\"modelperm-094c24ef7c734e77ae3d155af7783c2a\",\"object\":\"model_permission\",\"created\":1715248640,\"allow_create_engine\":false,\"allow_sampling\":true,\"allow_logprobs\":true,\"allow_search_indices\":false,\"allow_view\":true,\"allow_fine_tuning\":false,\"organization\":\"*\",\"group\":null,\"is_blocking\":false}]}]}"
     ]
    }
   ],
   "source": [
    "!curl http://0.0.0.0:8000/v1/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import concurrent.futures\n",
    "import time\n",
    "\n",
    "client = OpenAI(base_url=\"http://0.0.0.0:8000/v1\", api_key=\"sk-xxx\") # dummy key\n",
    "\n",
    "\n",
    "def get_response(name, max_tokens = 100, prompt = \"Write a story about llamas.\", system_prompt = \"You are a story writing assistant.\"):\n",
    "    t0 = time.time()\n",
    "    response = client.chat.completions.create(\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=0.1,\n",
    "        model=\"TheBloke/Mistral-7B-Instruct-v0.2-AWQ\",\n",
    "        messages=[\n",
    "            #{\"role\": \"system\", \"content\": system_prompt}, MISTRAL does not use a system prompt\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": system_prompt+\" \"+prompt\n",
    "            }\n",
    "        ])\n",
    "    return name, response, time.time() - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 7.250380277633667s\n",
      " To download and run Llama, a large language model from Hugging Face, you can follow these steps:\n",
      "\n",
      "1. First, make sure you have Python installed on your system. You can download it from the official website: https://www.python.org/downloads/\n",
      "\n",
      "2. Next, install the Transformers library, which is a Hugging Face library for state-of-the-art Natural Language Processing. You can install it using pip:\n",
      "\n",
      "```\n",
      "pip install transformers\n",
      "```\n",
      "\n",
      "3. Once the installation is complete, you can download the Llama model using the following command:\n",
      "\n",
      "```\n",
      "wget https://huggingface.co/LLM-Math/Llama-7B-hf/resolve/main/config.json\n",
      "wget https://huggingface.co/LLM-Math/Llama-7B-hf/resolve/main/tokenizer_config.json\n",
      "wget https://huggingface.co/LLM-Math/Llama-7B-hf/resolve/main/pytorch_model.bin\n",
      "wget https://h\n"
     ]
    }
   ],
   "source": [
    "name, response, t = get_response(\n",
    "    0, max_tokens=250, \n",
    "    system_prompt='You are expert code assistant. Please reply to user queries in an accurate, clear and concise manner.',\n",
    "    prompt = 'Tell me how I can download and run llama from huggingface.')\n",
    "print(f'Took {t}s')\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submited thread 0\n",
      "Submited thread 1\n",
      "Submited thread 2\n",
      "Submited thread 3\n",
      "Submited thread 4\n",
      "Submited thread 5\n",
      "Submited thread 6\n",
      "Submited thread 7\n",
      "Submited thread 8\n",
      "Submited thread 9\n",
      "Request 3, time: 9.10, n_tokens = 273\n",
      "Request 9, time: 9.08, n_tokens = 273\n",
      "Request 0, time: 9.12, n_tokens = 273\n",
      "Request 4, time: 9.10, n_tokens = 273\n",
      "Request 7, time: 9.09, n_tokens = 273\n",
      "Request 6, time: 9.10, n_tokens = 273\n",
      "Request 8, time: 9.09, n_tokens = 273\n",
      "Request 1, time: 9.12, n_tokens = 273\n",
      "Request 2, time: 9.11, n_tokens = 273\n",
      "Request 5, time: 9.10, n_tokens = 273\n",
      "Total Execution time: 9.125575065612793\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "threads = []\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    # Submit the tasks to the executor\n",
    "    for i in range(10):\n",
    "        threads.append(executor.submit(get_response, i, 250))\n",
    "        print('Submited thread', i)\n",
    "\n",
    "    # print results as and when they come in\n",
    "    for task in concurrent.futures.as_completed(threads):\n",
    "        name, response, exec_time = task.result()\n",
    "        message = response.choices[0].message.content\n",
    "        print(f'Request {name}, time: {exec_time:.2f}, n_tokens = {response.usage.total_tokens}')\n",
    "        #print(message)\n",
    "\n",
    "print('Total Execution time:', time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
